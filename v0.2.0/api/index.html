<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>API Reference ·   </title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><a href="../index.html"><img class="logo" src="../assets/logo.png" alt="   logo"/></a><h1>  </h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">Introduction</a></li><li class="current"><a class="toctext" href>API Reference</a><ul class="internal"></ul></li></ul></nav><article id="docs"><header><nav><ul><li><a href>API Reference</a></li></ul><a class="edit-page" href="https://github.com/zgornel/EmbeddingsAnalysis.jl/blob/master/docs/src/api.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>API Reference</span><a class="fa fa-bars" href="#"></a></div></header><ul><li><a href="#Base.size-Tuple{CompressedWordVectors}"><code>Base.size</code></a></li><li><a href="#EmbeddingsAnalysis.analogy-Union{Tuple{H}, Tuple{S}, Tuple{T}, Tuple{D}, Tuple{U}, Tuple{Q}, Tuple{CompressedWordVectors{Q,U,D,T,S,H},AbstractArray,AbstractArray}, Tuple{CompressedWordVectors{Q,U,D,T,S,H},AbstractArray,AbstractArray,Any}} where H where S where T where D where U where Q"><code>EmbeddingsAnalysis.analogy</code></a></li><li><a href="#EmbeddingsAnalysis.compress-Union{Tuple{WordVectors{S,T,H}}, Tuple{H}, Tuple{T}, Tuple{S}} where H where T where S"><code>EmbeddingsAnalysis.compress</code></a></li><li><a href="#EmbeddingsAnalysis.compressedwordvectors-Union{Tuple{T}, Tuple{AbstractString,Type{T}}} where T&lt;:Real"><code>EmbeddingsAnalysis.compressedwordvectors</code></a></li><li><a href="#EmbeddingsAnalysis.conceptnet2wv-Union{Tuple{E}, Tuple{K}, Tuple{L}, Tuple{ConceptNet{L,K,E},Language}} where E&lt;:AbstractFloat where K&lt;:AbstractString where L&lt;:Languages.Language"><code>EmbeddingsAnalysis.conceptnet2wv</code></a></li><li><a href="#EmbeddingsAnalysis.cosine"><code>EmbeddingsAnalysis.cosine</code></a></li><li><a href="#EmbeddingsAnalysis.cosine_similar_words"><code>EmbeddingsAnalysis.cosine_similar_words</code></a></li><li><a href="#EmbeddingsAnalysis.cosine_vec"><code>EmbeddingsAnalysis.cosine_vec</code></a></li><li><a href="#EmbeddingsAnalysis.in_vocabulary-Tuple{CompressedWordVectors,AbstractString}"><code>EmbeddingsAnalysis.in_vocabulary</code></a></li><li><a href="#EmbeddingsAnalysis.index-Tuple{CompressedWordVectors,Any}"><code>EmbeddingsAnalysis.index</code></a></li><li><a href="#EmbeddingsAnalysis.pca_reduction-Union{Tuple{WordVectors{S,T,H}}, Tuple{H}, Tuple{T}, Tuple{S}, Tuple{WordVectors{S,T,H},Int64}, Tuple{WordVectors{S,T,H},Int64,Int64}} where H&lt;:Integer where T&lt;:Real where S&lt;:AbstractString"><code>EmbeddingsAnalysis.pca_reduction</code></a></li><li><a href="#EmbeddingsAnalysis.similarity-Tuple{CompressedWordVectors,Any,Any}"><code>EmbeddingsAnalysis.similarity</code></a></li><li><a href="#EmbeddingsAnalysis.similarity_order-Union{Tuple{WordVectors{S,T,H}}, Tuple{H}, Tuple{T}, Tuple{S}, Tuple{WordVectors{S,T,H},T}} where H&lt;:Integer where T&lt;:Real where S&lt;:AbstractString"><code>EmbeddingsAnalysis.similarity_order</code></a></li><li><a href="#EmbeddingsAnalysis.vocab_reduction-Tuple{WordVectors,Any,Any}"><code>EmbeddingsAnalysis.vocab_reduction</code></a></li><li><a href="#EmbeddingsAnalysis.write2disk-Union{Tuple{H}, Tuple{T}, Tuple{S}, Tuple{IO,WordVectors{S,T,H}}} where H&lt;:Integer where T&lt;:Real where S&lt;:AbstractString"><code>EmbeddingsAnalysis.write2disk</code></a></li><li><a href="#EmbeddingsAnalysis.write2disk-Union{Tuple{H}, Tuple{S}, Tuple{T}, Tuple{D}, Tuple{U}, Tuple{Q}, Tuple{IO,CompressedWordVectors{Q,U,D,T,S,H}}} where H where S where T where D where U where Q"><code>EmbeddingsAnalysis.write2disk</code></a></li><li><a href="#Word2Vec.analogy_words"><code>Word2Vec.analogy_words</code></a></li><li><a href="#Word2Vec.get_vector-Tuple{CompressedWordVectors,Any}"><code>Word2Vec.get_vector</code></a></li><li><a href="#Word2Vec.vocabulary-Tuple{CompressedWordVectors}"><code>Word2Vec.vocabulary</code></a></li></ul><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="EmbeddingsAnalysis.compress-Union{Tuple{WordVectors{S,T,H}}, Tuple{H}, Tuple{T}, Tuple{S}} where H where T where S" href="#EmbeddingsAnalysis.compress-Union{Tuple{WordVectors{S,T,H}}, Tuple{H}, Tuple{T}, Tuple{S}} where H where T where S"><code>EmbeddingsAnalysis.compress</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">compress(wv [;kwargs...])</code></pre><p>Compresses <code>wv::WordVectors</code> by using array quantization.</p><p><strong>Keyword arguments</strong></p><ul><li><code>sampling_ratio::AbstractFloat</code> specifies the percentage of vectors to use</li></ul><p>for quantization codebook creation</p><ul><li><code>k::Int</code> number of quantization values for a codebook</li><li><code>m::Int</code> number of codebooks to use</li><li><code>method::Symbol</code> specifies the array quantization method</li><li><code>distance::PreMetric</code> is the distance</li></ul><p>Other keyword arguments specific to the quantization methods can also be provided.</p></div></div><a class="source-link" target="_blank" href="https://github.com/zgornel/EmbeddingsAnalysis.jl/blob/8a5562a08e8b11466cfa179cde59872b753c1a09/src/cwv.jl#L26-L40">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="EmbeddingsAnalysis.compressedwordvectors-Union{Tuple{T}, Tuple{AbstractString,Type{T}}} where T&lt;:Real" href="#EmbeddingsAnalysis.compressedwordvectors-Union{Tuple{T}, Tuple{AbstractString,Type{T}}} where T&lt;:Real"><code>EmbeddingsAnalysis.compressedwordvectors</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">compressedwordvectors(filename [,type=Float64][; kind=:text])</code></pre><p>Generate a <code>CompressedWordVectors</code> type object from a file.</p><p><strong>Arguments</strong></p><ul><li><code>filename::AbstractString</code> the embeddings file name</li><li><code>type::Type</code> type of the embedding vector elements; default <code>Float64</code></li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>kind::Symbol</code> specifies whether the embeddings file is textual (<code>:text</code>)</li></ul><p>or binary (<code>:binary</code>); default <code>:text</code></p></div></div><a class="source-link" target="_blank" href="https://github.com/zgornel/EmbeddingsAnalysis.jl/blob/8a5562a08e8b11466cfa179cde59872b753c1a09/src/cwv.jl#L193-L205">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="EmbeddingsAnalysis.conceptnet2wv-Union{Tuple{E}, Tuple{K}, Tuple{L}, Tuple{ConceptNet{L,K,E},Language}} where E&lt;:AbstractFloat where K&lt;:AbstractString where L&lt;:Languages.Language" href="#EmbeddingsAnalysis.conceptnet2wv-Union{Tuple{E}, Tuple{K}, Tuple{L}, Tuple{ConceptNet{L,K,E},Language}} where E&lt;:AbstractFloat where K&lt;:AbstractString where L&lt;:Languages.Language"><code>EmbeddingsAnalysis.conceptnet2wv</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">conceptnet2wv(cptnet, language)</code></pre><p>Converts a <code>ConceptNet</code> object, <code>cptnet</code> to a <code>WordVectors</code> object. The <code>language</code> of the word embeddings has to be specified explicitly as a <code>Symbol</code> or <code>Languages.Language</code> (Conceptnet embeddings can be multilingual).</p></div></div><a class="source-link" target="_blank" href="https://github.com/zgornel/EmbeddingsAnalysis.jl/blob/8a5562a08e8b11466cfa179cde59872b753c1a09/src/conceptnet2wv.jl#L1-L8">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="EmbeddingsAnalysis.cosine_vec" href="#EmbeddingsAnalysis.cosine_vec"><code>EmbeddingsAnalysis.cosine_vec</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">cosine_vec(wv::WordVectors, wordvector, n=10 [;vocab=nothing])</code></pre><p>Compute the cosine similarities and return best <code>n</code> positions and calculated values between <code>wordvector</code> and the word vectors from <code>wv</code>. A vocabulary mask <code>vocab</code> can be specified to consider only a subset of word vectors.</p></div></div><a class="source-link" target="_blank" href="https://github.com/zgornel/EmbeddingsAnalysis.jl/blob/8a5562a08e8b11466cfa179cde59872b753c1a09/src/utils.jl#L1-L8">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="EmbeddingsAnalysis.pca_reduction-Union{Tuple{WordVectors{S,T,H}}, Tuple{H}, Tuple{T}, Tuple{S}, Tuple{WordVectors{S,T,H},Int64}, Tuple{WordVectors{S,T,H},Int64,Int64}} where H&lt;:Integer where T&lt;:Real where S&lt;:AbstractString" href="#EmbeddingsAnalysis.pca_reduction-Union{Tuple{WordVectors{S,T,H}}, Tuple{H}, Tuple{T}, Tuple{S}, Tuple{WordVectors{S,T,H},Int64}, Tuple{WordVectors{S,T,H},Int64,Int64}} where H&lt;:Integer where T&lt;:Real where S&lt;:AbstractString"><code>EmbeddingsAnalysis.pca_reduction</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">pca_reduction(wv::WordVectors, rdim=7, outdim=size(wv.vectors,1); [do_pca=true])</code></pre><p>Post-processes word embeddings <code>wv</code> by removing the first <code>rdim</code> PCA components from the word vectors and also reduces the dimensionality to <code>outdim</code> through a subsequent PCA transform, if <code>do_pca=true</code>.</p><p><strong>Arguments</strong></p><ul><li><code>wv::WordVectors</code> the word embeddings</li><li><code>rdim::Int</code> the number of PCA components to remove from the data  (default 7)</li><li><code>outdim::Int</code> the output dimensionality of the data after the PCA  dimensionality reduction; it is performed only if <code>do_pca=true</code>  and the default value is the same as that of the input embeddings  i.e. no reduction</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>do_pca::Bool</code> whether to perform a PCA transform of the  post-processed data (default <code>true</code>)</li></ul><p><strong>References:</strong></p><ul><li><a href="https://arxiv.org/abs/1708.03629">Vikas Raunak &quot;Simple and effective dimensionality reduction for  word embeddings&quot;, NIPS 2017 Workshop</a></li></ul></div></div><a class="source-link" target="_blank" href="https://github.com/zgornel/EmbeddingsAnalysis.jl/blob/8a5562a08e8b11466cfa179cde59872b753c1a09/src/pca_reduction.jl#L1-L24">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="EmbeddingsAnalysis.similarity_order-Union{Tuple{WordVectors{S,T,H}}, Tuple{H}, Tuple{T}, Tuple{S}, Tuple{WordVectors{S,T,H},T}} where H&lt;:Integer where T&lt;:Real where S&lt;:AbstractString" href="#EmbeddingsAnalysis.similarity_order-Union{Tuple{WordVectors{S,T,H}}, Tuple{H}, Tuple{T}, Tuple{S}, Tuple{WordVectors{S,T,H},T}} where H&lt;:Integer where T&lt;:Real where S&lt;:AbstractString"><code>EmbeddingsAnalysis.similarity_order</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">similarity_order(wv::WordVectors, alpha=-0.65)</code></pre><p>Post-processes the word embeddings <code>wv</code> so that the embeddings capture more information than directly apparent through a linear transformation that adjusts the similarity order of the model. The function returns a new <code>WordVectors</code> object containing the processed embeddings.</p><p><strong>Arguments</strong></p><ul><li><code>wv::WordVectors</code> the word embeddings</li></ul><p><strong><code>alpha::AbstractFloat</code> the <code>α</code> parameter of the algorithm (default -0.65)</strong></p><p><strong>References:</strong></p><ul><li><a href="https://arxiv.org/pdf/1809.02094.pdf">Artetxe et al. &quot;Uncovering divergent linguistic information in  word embeddings with lessons for intrinsic and extrinsic evaluation&quot;,  2018</a></li></ul></div></div><a class="source-link" target="_blank" href="https://github.com/zgornel/EmbeddingsAnalysis.jl/blob/8a5562a08e8b11466cfa179cde59872b753c1a09/src/similarity_order.jl#L1-L17">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="EmbeddingsAnalysis.vocab_reduction-Tuple{WordVectors,Any,Any}" href="#EmbeddingsAnalysis.vocab_reduction-Tuple{WordVectors,Any,Any}"><code>EmbeddingsAnalysis.vocab_reduction</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">vocab_reduction(wv::WordVectors, seed, nn)</code></pre><p>Produces a reduced vocabulary version of <code>wv</code> by removing all but the <code>nn</code> nearest neighbors of each word present in the vocabulary <code>seed</code>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/zgornel/EmbeddingsAnalysis.jl/blob/8a5562a08e8b11466cfa179cde59872b753c1a09/src/vocab_reduction.jl#L1-L7">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="EmbeddingsAnalysis.write2disk-Union{Tuple{H}, Tuple{S}, Tuple{T}, Tuple{D}, Tuple{U}, Tuple{Q}, Tuple{IO,CompressedWordVectors{Q,U,D,T,S,H}}} where H where S where T where D where U where Q" href="#EmbeddingsAnalysis.write2disk-Union{Tuple{H}, Tuple{S}, Tuple{T}, Tuple{D}, Tuple{U}, Tuple{Q}, Tuple{IO,CompressedWordVectors{Q,U,D,T,S,H}}} where H where S where T where D where U where Q"><code>EmbeddingsAnalysis.write2disk</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">write2disk(filename::AbstractString, wv::CompressedWordVectors [;kind=:binary])</code></pre><p>Writes compressed embeddings to disk.</p><p><strong>Arguments</strong></p><ul><li><code>filename::AbstractString</code> the embeddings file name</li><li><code>wv::CompressedWordVectors</code> the embeddings</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>kind::Symbol</code> specifies whether the embeddings file is textual (<code>:text</code>)</li></ul><p>or binary (<code>:binary</code>); default <code>:binary</code></p></div></div><a class="source-link" target="_blank" href="https://github.com/zgornel/EmbeddingsAnalysis.jl/blob/8a5562a08e8b11466cfa179cde59872b753c1a09/src/write2disk.jl#L59-L71">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="EmbeddingsAnalysis.write2disk-Union{Tuple{H}, Tuple{T}, Tuple{S}, Tuple{IO,WordVectors{S,T,H}}} where H&lt;:Integer where T&lt;:Real where S&lt;:AbstractString" href="#EmbeddingsAnalysis.write2disk-Union{Tuple{H}, Tuple{T}, Tuple{S}, Tuple{IO,WordVectors{S,T,H}}} where H&lt;:Integer where T&lt;:Real where S&lt;:AbstractString"><code>EmbeddingsAnalysis.write2disk</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">write2disk(filename::AbstractString, wv::WordVectors [;kind=:binary])</code></pre><p>Writes embeddings to disk.</p><p><strong>Arguments</strong></p><ul><li><code>filename::AbstractString</code> the embeddings file name</li><li><code>wv::WordVectors</code> the embeddings</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>kind::Symbol</code> specifies whether the embeddings file is textual (<code>:text</code>)</li></ul><p>or binary (<code>:binary</code>); default <code>:binary</code></p></div></div><a class="source-link" target="_blank" href="https://github.com/zgornel/EmbeddingsAnalysis.jl/blob/8a5562a08e8b11466cfa179cde59872b753c1a09/src/write2disk.jl#L1-L13">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Word2Vec.analogy_words" href="#Word2Vec.analogy_words"><code>Word2Vec.analogy_words</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">analogy_words(cwv, pos, neg, n=5)</code></pre><p>Return the top <code>n</code> words computed by analogy similarity between positive words <code>pos</code> and negaive words <code>neg</code>. from the CompressedWordVectors <code>cwv</code>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/zgornel/EmbeddingsAnalysis.jl/blob/8a5562a08e8b11466cfa179cde59872b753c1a09/src/cwv.jl#L180-L186">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Word2Vec.get_vector-Tuple{CompressedWordVectors,Any}" href="#Word2Vec.get_vector-Tuple{CompressedWordVectors,Any}"><code>Word2Vec.get_vector</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">get_vector(cwv, word)</code></pre><p>Return the vector representation of <code>word</code> from the CompressedWordVectors <code>cwv</code>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/zgornel/EmbeddingsAnalysis.jl/blob/8a5562a08e8b11466cfa179cde59872b753c1a09/src/cwv.jl#L96-L100">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Word2Vec.vocabulary-Tuple{CompressedWordVectors}" href="#Word2Vec.vocabulary-Tuple{CompressedWordVectors}"><code>Word2Vec.vocabulary</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">vocabulary(cwv)</code></pre><p>Return the vocabulary as a vector of words of the CompressedWordVectors <code>cwv</code>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/zgornel/EmbeddingsAnalysis.jl/blob/8a5562a08e8b11466cfa179cde59872b753c1a09/src/cwv.jl#L63-L67">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Base.size-Tuple{CompressedWordVectors}" href="#Base.size-Tuple{CompressedWordVectors}"><code>Base.size</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">size(cwv)</code></pre><p>Return the word vector length and the number of words as a tuple.</p></div></div><a class="source-link" target="_blank" href="https://github.com/zgornel/EmbeddingsAnalysis.jl/blob/8a5562a08e8b11466cfa179cde59872b753c1a09/src/cwv.jl#L80-L84">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="EmbeddingsAnalysis.analogy-Union{Tuple{H}, Tuple{S}, Tuple{T}, Tuple{D}, Tuple{U}, Tuple{Q}, Tuple{CompressedWordVectors{Q,U,D,T,S,H},AbstractArray,AbstractArray}, Tuple{CompressedWordVectors{Q,U,D,T,S,H},AbstractArray,AbstractArray,Any}} where H where S where T where D where U where Q" href="#EmbeddingsAnalysis.analogy-Union{Tuple{H}, Tuple{S}, Tuple{T}, Tuple{D}, Tuple{U}, Tuple{Q}, Tuple{CompressedWordVectors{Q,U,D,T,S,H},AbstractArray,AbstractArray}, Tuple{CompressedWordVectors{Q,U,D,T,S,H},AbstractArray,AbstractArray,Any}} where H where S where T where D where U where Q"><code>EmbeddingsAnalysis.analogy</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">analogy(cwv, pos, neg, n=5)</code></pre><p>Compute the analogy similarity between two lists of words. The positions and the similarity values of the top <code>n</code> similar words will be returned. For example, <code>king - man + woman = queen</code> will be <code>pos=[&quot;king&quot;, &quot;woman&quot;], neg=[&quot;man&quot;]</code>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/zgornel/EmbeddingsAnalysis.jl/blob/8a5562a08e8b11466cfa179cde59872b753c1a09/src/cwv.jl#L141-L149">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="EmbeddingsAnalysis.cosine" href="#EmbeddingsAnalysis.cosine"><code>EmbeddingsAnalysis.cosine</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">cosine(cwv, word, n=10)</code></pre><p>Return the position of <code>n</code> (by default <code>n = 10</code>) neighbors of <code>word</code> and their cosine similarities.</p></div></div><a class="source-link" target="_blank" href="https://github.com/zgornel/EmbeddingsAnalysis.jl/blob/8a5562a08e8b11466cfa179cde59872b753c1a09/src/cwv.jl#L105-L110">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="EmbeddingsAnalysis.cosine_similar_words" href="#EmbeddingsAnalysis.cosine_similar_words"><code>EmbeddingsAnalysis.cosine_similar_words</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">cosine_similar_words(cwv, word, n=10)</code></pre><p>Return the top <code>n</code> (by default <code>n = 10</code>) most similar words to <code>word</code> from the CompressedWordVectors <code>cwv</code>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/zgornel/EmbeddingsAnalysis.jl/blob/8a5562a08e8b11466cfa179cde59872b753c1a09/src/cwv.jl#L129-L134">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="EmbeddingsAnalysis.in_vocabulary-Tuple{CompressedWordVectors,AbstractString}" href="#EmbeddingsAnalysis.in_vocabulary-Tuple{CompressedWordVectors,AbstractString}"><code>EmbeddingsAnalysis.in_vocabulary</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">in_vocabulary(cwv, word)</code></pre><p>Return <code>true</code> if <code>word</code> is part of the vocabulary of the CompressedWordVector <code>cwv</code> and <code>false</code> otherwise.</p></div></div><a class="source-link" target="_blank" href="https://github.com/zgornel/EmbeddingsAnalysis.jl/blob/8a5562a08e8b11466cfa179cde59872b753c1a09/src/cwv.jl#L71-L76">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="EmbeddingsAnalysis.index-Tuple{CompressedWordVectors,Any}" href="#EmbeddingsAnalysis.index-Tuple{CompressedWordVectors,Any}"><code>EmbeddingsAnalysis.index</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">index(cwv, word)</code></pre><p>Return the index of <code>word</code> from the CompressedWordVectors <code>cwv</code>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/zgornel/EmbeddingsAnalysis.jl/blob/8a5562a08e8b11466cfa179cde59872b753c1a09/src/cwv.jl#L88-L92">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="EmbeddingsAnalysis.similarity-Tuple{CompressedWordVectors,Any,Any}" href="#EmbeddingsAnalysis.similarity-Tuple{CompressedWordVectors,Any,Any}"><code>EmbeddingsAnalysis.similarity</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">similarity(cwv, word1, word2)</code></pre><p>Return the cosine similarity value between two words <code>word1</code> and <code>word2</code>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/zgornel/EmbeddingsAnalysis.jl/blob/8a5562a08e8b11466cfa179cde59872b753c1a09/src/cwv.jl#L119-L123">source</a></section><footer><hr/><a class="previous" href="../"><span class="direction">Previous</span><span class="title">Introduction</span></a></footer></article></body></html>
